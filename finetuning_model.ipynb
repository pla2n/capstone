{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd7507-c099-449b-bf5e-743cdce6de6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d61e2-b774-4f76-a7da-3d0b51b6b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdf5f6-dc31-4341-a833-d486af9ff7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "# 모델 체크포인트\n",
    "checkpoint = \"/workspace/LSY/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(\"start load model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"start load tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eecd2ee-7d87-4a39-b74b-d5b31d6ff8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 3만 개의 데이터셋 로드 (경로는 사용자 환경에 맞게 설정)\n",
    "#raw_datasets = load_dataset(\"allenai/scitldr\", \"Abstract\")\n",
    "\n",
    "raw_datasets = load_dataset(\"/workspace/LSY/4-1/arxiv-summarization dataset\")\n",
    "\n",
    "# if DEBUG :\n",
    "#     # 데이터 섞고 앞에서 5만 개 선택\n",
    "#     train_subset = raw_datasets[\"train\"].shuffle(seed=42).select(range(3))\n",
    "    \n",
    "#     # 검증, 테스트는 유지\n",
    "#     validation_subset = raw_datasets[\"validation\"].shuffle(seed=42).select(range(3))\n",
    "#     test_subset = raw_datasets[\"test\"].shuffle(seed=42).select(range(3))\n",
    "    \n",
    "# else :\n",
    "#     train_subset = raw_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "#     validation_subset = raw_datasets[\"validation\"].shuffle(seed=42).select(range(100))\n",
    "#     test_subset = raw_datasets[\"test\"].shuffle(seed=42).select(range(100))\n",
    "train_subset = raw_datasets[\"train\"].shuffle(seed=40).select(range(10000))\n",
    "validation_subset = raw_datasets[\"validation\"].shuffle(seed=40).select(range(250))\n",
    "test_subset = raw_datasets[\"test\"].shuffle(seed=40).select(range(250))\n",
    "\n",
    "\n",
    "# 새로운 데이터셋 구성\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"validation\": validation_subset,\n",
    "    \"test\": test_subset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afb794-253e-4d49-b6fc-853ed1781875",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d78a3e-5ee8-477b-9938-a019591585cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the following article.\n",
    "\n",
    "### Article:\n",
    "{}\n",
    "\n",
    "### Summary:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# def preprocess(example):\n",
    "#     selected_sentences = [\n",
    "#         sent for sent, label in zip(example[\"source\"], example[\"source_labels\"]) if label == 1\n",
    "#     ]\n",
    "#     input_text = \" \".join(selected_sentences)\n",
    "    \n",
    "#     return {\n",
    "#         \"article\": input_text,\n",
    "#         \"abstract\": example[\"target\"]\n",
    "#     }\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    return {\n",
    "        \"text\": alpaca_prompt.format(example[\"article\"], example[\"abstract\"]) + EOS_TOKEN\n",
    "    }\n",
    "\n",
    "# 데이터셋 변환\n",
    "#datasets = datasets.map(preprocess)\n",
    "datasets = datasets.map(formatting_prompts_func)\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51354e-fcd3-42bb-8398-17cef6cd707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "def tokenize_function(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"text\"],\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b6824-6b07-4885-90ae-3ae3ae39541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0852247-9a3a-43bb-8402-771e5596800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfeaad0-9a06-4788-b16c-6a87c56afdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets.map(tokenize_function, remove_columns=datasets[\"train\"].column_names, batched=True)\n",
    "print(datasets)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febed411-c634-4df9-9424-71e027a16780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate  # ✅ 새 라이브러리\n",
    "import numpy as np\n",
    "# def compute_metrics(pred):\n",
    "    \n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "\n",
    "#     m1 = evaluate.load('accuracy')\n",
    "#     m2 = evaluate.load('f1')\n",
    "\n",
    "#     acc = m1.compute(predictions=preds, references=labels)['accuracy']\n",
    "#     f1 = m2.compute(predictions=preds, references=labels)['f1']\n",
    "\n",
    "#     return {'accuracy':acc, 'f1':f1}\n",
    "\n",
    "metric = evaluate.load('rouge')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.astype(np.uint16)\n",
    "    predictions = np.argmax(logits, axis=-1).astype(np.uint16)\n",
    "    predictions = tokenizer.batch_decode(predictions)\n",
    "    labels = tokenizer.batch_decode(labels)\n",
    "    if DEBUG :\n",
    "        print(\"\\n\\n====================\")\n",
    "        print(f\"{predictions=}\")\n",
    "        print(f\"\\n{labels=}\")\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863119c4-5daa-44f0-a0a0-ad6c570ecbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     inference_mode=True,\n",
    "#     r=16,  # Low-rank 매트릭스 차원 설정\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     lora_alpha=32,  # 학습된 LoRA 가중치의 스케일 조절\n",
    "#     lora_dropout=0.05,  # Dropout 적용\n",
    "#     bias=\"none\",\n",
    "#     use_rslora=False,\n",
    "#     loftq_config=None,\n",
    "# )\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.01,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655f7ba-b162-4946-a400-6c1007d549b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback, Trainer\n",
    "\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=datasets[\"train\"], \n",
    "    eval_dataset=datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=100)],\n",
    "\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=8,  # H100 80GB 기준 최적화\n",
    "        per_device_eval_batch_size = 1,  # 평가 시 배치 크기 감소 (기본 8 → 2)\n",
    "        gradient_accumulation_steps=4,   # 최소한의 Accumulation\n",
    "        num_train_epochs=100,  # 데이터가 적어졌으므로 학습 횟수 증가\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 2 if DEBUG else 100,\n",
    "        save_steps= 1000,\n",
    "        load_best_model_at_end = True,\n",
    "        learning_rate=2e-4,\n",
    "        bf16=True,\n",
    "        logging_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        seed=40,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_total_limit=3,\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f757cdd-2da9-4c4a-ba60-9b41c236e583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a05d03-a3dc-4388-beca-35dbec901335",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/workspace/LSY/4-1/pretrained/model_early100steps.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c55cb-1f6a-4362-9b5c-eb38b1bda4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(paper_text):\n",
    "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the following section of a research paper in 3 detailed sentences. The summary should include the main arguments, methodology, results, or conclusions of the section, while preserving important context and details. Base your summary on the text provided below.\n",
    "\n",
    "### Article:\n",
    "{paper_text}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=250)\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# 예제 논문 요약\n",
    "sample_paper = ''' This work is subject to limitations in two main\n",
    " aspects: (1) Limited Focus on LLM Bias in Me\n",
    "dia Bias Prediction: The scope of bias analysis is\n",
    " constrained by the availability of three-way (left-,\n",
    " center-, and right-leaning) labeled data. Our study\n",
    " relies on two political bias prediction datasets with\n",
    " three-way labels to investigate biases during LLM\n",
    " prediction. However, datasets with only biased and\n",
    " non-biased labels would not suffice for our analysis\n",
    " in this paper. (2) Assumption of Ground Truth: We\n",
    " operate under the assumption that human-labeled\n",
    " data serves as an unbiased ground truth for assess\n",
    "ing LLM biases. Nevertheless, human annotations\n",
    " are inherently subjective and may be influenced by\n",
    " individual biases, potentially impacting the validity\n",
    " of our evaluations'''\n",
    "summary = summarize_paper(sample_paper)\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e3aa4c-5c71-4a51-9139-a4eba772f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def sep_no(s):\n",
    "    if s[0] in ['I', 'V', 'X']:\n",
    "        tnum, txt = '', ''\n",
    "        dot_idx = -1\n",
    "        if '.' in s: # I.A or I.1\n",
    "            dot_idx = s.find('.')\n",
    "            tnum += s[:dot_idx+2]\n",
    "            txt = s[dot_idx+2:]\n",
    "        elif '-' in s: # I-A or I-1\n",
    "            dot_idx = s.find('-')\n",
    "            tnum += s[:dot_idx+2]\n",
    "            txt = s[dot_idx+2:]\n",
    "        else:\n",
    "            # I II III IV V VI VII VIII IX X\n",
    "            if s.startswith('IIntroduction') or s.startswith('IINTRODUCTION'):\n",
    "                tnum += 'I'\n",
    "                txt = 'Introduction'\n",
    "            elif s.startswith('III'): # 3\n",
    "                tnum += 'III'\n",
    "                txt = s[3:].strip()\n",
    "            elif s.startswith('II') and not s.startswith('IIntro'): # 2\n",
    "                tnum += 'II'\n",
    "                txt = s[2:].strip()\n",
    "            elif s.startswith('IX'): # 9\n",
    "                tnum += 'IX'\n",
    "                txt = s[1:].strip()\n",
    "            elif s.startswith('IV'): # 4\n",
    "                tnum += 'IV'\n",
    "                txt = s[2:].strip()\n",
    "            elif s.startswith('I'): # 1\n",
    "                tnum += 'I'\n",
    "                txt = s[1:].strip()\n",
    "            elif s.startswith('VIII'): # 8\n",
    "                tnum += 'VIII'\n",
    "                txt = s[4:].strip()\n",
    "            elif s.startswith('VII'): # 7\n",
    "                tnum += 'VII'\n",
    "                txt = s[3:].strip()\n",
    "            elif s.startswith('VI'): # 6\n",
    "                tnum += 'VI'\n",
    "                txt = s[2:].strip()\n",
    "            elif s.startswith('V'): # 5\n",
    "                tnum += 'V'\n",
    "                txt = s[1:].strip()\n",
    "            elif s.startswith('X'): # 10\n",
    "                tnum += 'X'\n",
    "                txt = s[1:].strip()\n",
    "            else:\n",
    "                txt = s\n",
    "\n",
    "        seperated = (tnum, txt)\n",
    "        return seperated\n",
    "    \n",
    "    elif s[0].isdigit():\n",
    "        dot_idx = 0\n",
    "        for i in s:\n",
    "            if i.isalpha():\n",
    "                break\n",
    "            dot_idx += 1\n",
    "        \n",
    "        tnum, txt = s[:dot_idx], s[dot_idx:]\n",
    "        seperated = (tnum, txt)\n",
    "        return seperated\n",
    "    \n",
    "    else: # maybe references section\n",
    "        tnum, txt = '0', s\n",
    "        seperated = (tnum, txt)\n",
    "        return seperated\n",
    "\n",
    "\n",
    "\n",
    "def extract(url):\n",
    "    if len(url) < 1:\n",
    "        print('error: invalid url', file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "    sections = []\n",
    "\n",
    "    now = {'title': 'default', 'paragraphs': []}\n",
    "\n",
    "    for element in soup.find_all(['h1', 'h2', 'h3', 'h6', 'p']):\n",
    "        if element.name == 'h6': # abstract\n",
    "            title_text = ('0', element.get_text(strip=True))\n",
    "            now = {\n",
    "                'title': title_text,\n",
    "                'paragraphs': []\n",
    "            }\n",
    "\n",
    "        elif element.name in ['h1', 'h2', 'h3', 'h6']: # title number + title name\n",
    "            if now['paragraphs']:\n",
    "                sections.append(now)\n",
    "            title_text = element.get_text(strip=True)\n",
    "            title_text = sep_no(title_text)\n",
    "            now = {\n",
    "                'title': title_text,\n",
    "                'paragraphs': []\n",
    "            }\n",
    "            #print(title_text)\n",
    "\n",
    "        elif element.name == 'p': # paragraph\n",
    "            paragraph = element.get_text(strip=True)\n",
    "\n",
    "            # except LaTeX expression\n",
    "            if paragraph and not re.search(r'[\\$]|\\\\\\(|\\\\\\)', paragraph):\n",
    "                now['paragraphs'].append(paragraph)\n",
    "\n",
    "    # last section\n",
    "    if now['paragraphs']:\n",
    "        sections.append(now)\n",
    "\n",
    "    return sections\n",
    "\n",
    "url = \"https://arxiv.org/html/2504.07495v1\"\n",
    "# execute\n",
    "sections = extract(url)\n",
    "\n",
    "if sections:\n",
    "    for sec in sections:\n",
    "        if sec['title'][1] != 'References':\n",
    "            print(sec['title'])\n",
    "            summary = summarize_paper('\\n'.join(sec['paragraphs']))\n",
    "            print(summary[len('\\n'.join(sec['paragraphs']))+256:])\n",
    "\n",
    "            sec['paragraphs'] = summary[len('\\n'.join(sec['paragraphs']))+271:-2]\n",
    "        print()\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8870d-6aba-48ca-a269-d45595d01c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import re\n",
    "\n",
    "def process_arxiv_url(url):\n",
    "    arxiv_id = re.search(r'arxiv\\.org/abs/([\\w\\.]+)', url).group(1)\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    for paper in client.results(search):\n",
    "        title = paper.title\n",
    "        abstract = paper.summary\n",
    "        with open(f\"{arxiv_id}.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# {title}\\n\\n## Abstract\\n{abstract}\")\n",
    "    return f\"{arxiv_id}.md\"\n",
    "\n",
    "# 받은 URL 처리\n",
    "md_file = process_arxiv_url(received_arxiv_url)\n",
    "print(f\"Markdown file created: {md_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a52e1-8dfa-447d-a439-3d5609ec6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_md(md_text, keyword):\n",
    "    os.makedirs(os.path.dirname(f'/workspace/LSY/4-1/arxiv-summarization/outputs/{keyword}_output.md'), exist_ok=True)\n",
    "    with open(f'/workspace/LSY/4-1/arxiv-summarization/outputs/{keyword}_output.md', 'w', encoding='utf-8') as file:\n",
    "        file.write(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cd944-2f59-433e-a219-833c59d20514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markdown(related_keywords):\n",
    "    \"\"\"\n",
    "    생성된 키워드와 설명을 마크다운 형식으로 변환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in related_keywords:\n",
    "        if i['title'] == 'default':\n",
    "            keyword = i['paragraphs']\n",
    "            markdown = f\"# {keyword}\\n\"\n",
    "        elif len(i['title'][0]) < 2:\n",
    "            markdown += f\" ## {i['title'][0]+ \". \" + i['title'][1]}\\n\"  # 하위 주제\n",
    "            markdown += f\" ### {i['paragraphs']}\\n\"  # 각 단어를 하위 키워드로 추가\n",
    "        else:\n",
    "            markdown += f\" #### {i['title'][0]+ \" \" + i['title'][1]}\\n\"  # 하위 주제\n",
    "            markdown += f\" ##### {i['paragraphs']}\\n\"  # 각 단어를 하위 키워드로 추가\n",
    "\n",
    "    extract_md(markdown, keyword)\n",
    "    \n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c66615-ed7a-4484-b446-5d4bfab52891",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = generate_markdown(sections)\n",
    "print(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85198836-c99c-48f4-bd69-68bbf0419efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
